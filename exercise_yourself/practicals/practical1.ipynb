{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('libraries imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Mask it with the *idx* list for easier access and use only slicing instead of a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
       "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
       "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
       "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
       "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
       "        [1, 2, 1, 3, 3, 1, 2, 1, 3, 3, 1, 2, 1],\n",
       "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.full(size=(13, 13), fill_value=1)\n",
    "# x.new_full(size = (2, 2), fill_value = 2)\n",
    "idx1 = [1, 6, 11] \n",
    "idx2 = [3, 4, 8, 9]\n",
    "x[idx1] = 2     ## access the 2nd, 7th, 12th row using the mask\n",
    "x[:, idx1] = 2  \n",
    "x[idx2, 3:5] = 3\n",
    "x[idx2, 8:10] = 3\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000+0.j, 20.0000+0.j, 19.0000+0.j, 18.0000+0.j,  2.0000+0.j,  3.0000+0.j,\n",
       "         4.0000+0.j, 17.0000+0.j, 16.0000+0.j, 15.0000+0.j,  5.0000+0.j,  6.0000+0.j,\n",
       "        14.0000+0.j, 13.0000+0.j, 12.0000+0.j,  7.0000+0.j,  8.0000+0.j, 11.0000+0.j,\n",
       "        10.0000+0.j,  9.0000+0.j], dtype=torch.complex128)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "square_matrix_size = (20, 20)\n",
    "\n",
    "x = torch.randn(square_matrix_size, dtype = torch.float64)  ## use same dtypes or the multiplicaion fails\n",
    "y = torch.diagflat(torch.arange(1, 21, dtype = torch.float64))\n",
    "\n",
    "z = torch.mm(torch.mm(x, y), x.inverse()) ## M * [1..20] diagonals * M_inverse\n",
    "\n",
    "result = torch.linalg.eigvals(z) ## directly computes the eigenvalues instead of torch.linalg.eig\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Flops per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000 x 5000) multiplication took 2.0359696999985317 seconds\n",
      "Estimated 491166445159.1402 floating point operations per second.\n",
      "With Result: tensor([[  15.4884,  -24.6764,   81.7239,  ...,   -9.3399,  -44.9573,\n",
      "          -30.7097],\n",
      "        [  46.1723,  -16.1209,   70.5612,  ...,   17.6019,    5.7808,\n",
      "         -108.8492],\n",
      "        [  41.6562,   75.4598,   10.2209,  ...,   -3.7426,  -17.2806,\n",
      "           22.4497],\n",
      "        ...,\n",
      "        [  70.2391,    3.2419,  -13.0488,  ...,  -14.0357,   -4.9220,\n",
      "          -36.7500],\n",
      "        [ -58.3854,   15.0983, -144.9469,  ...,  -27.2994,   58.5320,\n",
      "         -101.4557],\n",
      "        [  44.7101, -132.8069,  -49.0600,  ...,  -95.6766,   -0.7823,\n",
      "           70.8347]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "from torch import square\n",
    "time_start = perf_counter()\n",
    "\n",
    "square_matrix_size = (5000, 5000)\n",
    "\n",
    "x = torch.randn(square_matrix_size, dtype = torch.float64)\n",
    "y = torch.randn(square_matrix_size, dtype = torch.float64)\n",
    "\n",
    "z = torch.mm(x, y)\n",
    "flops_estimate = pow(2 * square_matrix_size[0], 3) ## case for a MM multiplication\n",
    "\n",
    "time_stop = perf_counter()\n",
    "total_time = time_stop - time_start\n",
    "\n",
    "print('({} x {}) multiplication took {} seconds'.format(square_matrix_size[0], square_matrix_size[1], total_time))\n",
    "print('Estimated {} floating point operations per second.'.format(flops_estimate / total_time))\n",
    "print('With Result: {}'.format(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Playing with strides\n",
    "\n",
    "Building the same function that performs a row multiplication, one time using for loops and plain Python and the other time using tensors in PyTorch, to ultimately check the time difference as a performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_row(t1): ## t1 -> tensor\n",
    "    if (t1.dim() > 2):\n",
    "        print('Tensor has more than two dimensions!')\n",
    "        raise OverflowError\n",
    "    else:\n",
    "        flattened_tensor = t1.view(-1)\n",
    "        total_size = flattened_tensor.size(dim=0)\n",
    "        multiplying_factor = 1.0\n",
    "        \n",
    "        for i in range(1, total_size):\n",
    "            if (i % t1.size()[1]  == 0):\n",
    "                multiplying_factor += 1.0\n",
    "            flattened_tensor[i] *= multiplying_factor\n",
    "    \n",
    "    result = flattened_tensor.view(t1.size()) ## back to original shape\n",
    "    return result\n",
    "\n",
    "def mul_row_fast(t1):\n",
    "    n_rows = t1.size()[0]\n",
    "    \n",
    "    t2 = torch.arange(1.0, n_rows + 1).view(n_rows, 1) ## t2 can be seen as the multiplicaton tensor\n",
    "    result = torch.mul(t1, t2) \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the performance time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000e+00, 2.0000e+00, 2.0000e+00,  ..., 2.0000e+00, 2.0000e+00,\n",
      "         2.0000e+00],\n",
      "        [4.0000e+00, 4.0000e+00, 4.0000e+00,  ..., 4.0000e+00, 4.0000e+00,\n",
      "         4.0000e+00],\n",
      "        [6.0000e+00, 6.0000e+00, 6.0000e+00,  ..., 6.0000e+00, 6.0000e+00,\n",
      "         6.0000e+00],\n",
      "        ...,\n",
      "        [3.9960e+03, 3.9960e+03, 3.9960e+03,  ..., 3.9960e+03, 3.9960e+03,\n",
      "         3.9960e+03],\n",
      "        [3.9980e+03, 3.9980e+03, 3.9980e+03,  ..., 3.9980e+03, 3.9980e+03,\n",
      "         3.9980e+03],\n",
      "        [4.0000e+03, 4.0000e+03, 4.0000e+03,  ..., 4.0000e+03, 4.0000e+03,\n",
      "         4.0000e+03]])\n",
      "Original row multiplication took 16.533173599998918 seconds\n",
      "\n",
      "tensor([[2.0000e+00, 2.0000e+00, 2.0000e+00,  ..., 2.0000e+00, 2.0000e+00,\n",
      "         2.0000e+00],\n",
      "        [8.0000e+00, 8.0000e+00, 8.0000e+00,  ..., 8.0000e+00, 8.0000e+00,\n",
      "         8.0000e+00],\n",
      "        [1.8000e+01, 1.8000e+01, 1.8000e+01,  ..., 1.8000e+01, 1.8000e+01,\n",
      "         1.8000e+01],\n",
      "        ...,\n",
      "        [7.9840e+06, 7.9840e+06, 7.9840e+06,  ..., 7.9840e+06, 7.9840e+06,\n",
      "         7.9840e+06],\n",
      "        [7.9920e+06, 7.9920e+06, 7.9920e+06,  ..., 7.9920e+06, 7.9920e+06,\n",
      "         7.9920e+06],\n",
      "        [8.0000e+06, 8.0000e+06, 8.0000e+06,  ..., 8.0000e+06, 8.0000e+06,\n",
      "         8.0000e+06]])\n",
      "Fast PyTorch row multiplication took 0.002851600000212784 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "m = torch.full((2000, 800), 2.0)\n",
    "\n",
    "time_start = perf_counter()\n",
    "print(mul_row(m))\n",
    "time_stop = perf_counter()    \n",
    "total_time = time_stop - time_start\n",
    "print('Original row multiplication took {} seconds'.format(total_time))\n",
    "\n",
    "print('')\n",
    "\n",
    "time_start = perf_counter()\n",
    "print(mul_row_fast(m))\n",
    "time_stop = perf_counter()    \n",
    "total_time = time_stop - time_start\n",
    "print('Fast PyTorch row multiplication took {} seconds'.format(total_time))      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6290ab70aa2c9e6859d722745d4fdeafb895ca1190e93c7ac9c8d926153eb965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
